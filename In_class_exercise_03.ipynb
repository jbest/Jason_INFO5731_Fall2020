{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In-class-exercise-03.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPq6x6du7-5q"
      },
      "source": [
        "## The third In-class-exercise (9/16/2020, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI2w-mF37-5y"
      },
      "source": [
        "The purpose of this exercise is to under users' information needs, then collect the data for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ4-NYJx7-5z"
      },
      "source": [
        "Question 1 (8 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xnv1Mua7-51"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Digital images of preserved plant specimens are used in digitization worflows to allow people to view specimen images\n",
        "and transcribe the written plant data from the specimen sheet into structured metadata fields which align with the\n",
        "Darwin Core metadata standard. Millions of such human-parsed records exist along with high resolution specimen images.\n",
        "Many of these images have clear, high-quality typewritten text that is very amenable to text extraction using OCR.\n",
        "I propose a project which uses OCR on a representative sample of images to extract raw text from each image then\n",
        "correlates the parsed, structured metadata records with the raw text records to evaluate the suitability of the dataset \n",
        "for development into a plant specimen text corpus.\n",
        "\n",
        "This corpus can then be used to develop and train NLP workflows and models to extract critical information from the \n",
        "raw OCR text of new records. The information to be extracted includes named entities, locations, dates, and scientific\n",
        "names among others.\n",
        "\n",
        "Specimen Records in Darwin Core Format\n",
        "Specimen records will be selected from various herbarium collection datasets that have both specimen records and\n",
        "images available online through the Symbiota web platform (e.g. https://portal.torcherbaria.org/). Records with \n",
        "associated image records will be retrieved using a Python script that allows different query parameters to be used\n",
        "to select records across different herbaria, botanists, species, locations, etc. The specimen records are structured\n",
        "in Darwin Core format and the image records are structured in Audubon Core format which includes the URL of images\n",
        "allowing the full-resolution image for each record to be retrieved. Approximately 2000 specimen records will be retrieved\n",
        "at this stage.\n",
        "\n",
        "Specimen Images and OCR\n",
        "Specimen image records acquired through the specimen data download process include URLs of the specimen images related\n",
        "to each specimen record. These images will be retrieved using a script to consolidate them into local data storage.\n",
        "All images will be processed to extract text using the OCR service in the Google Cloud Vision platform. The results of\n",
        "the OCR process will be analyzed to determine which specimens produced clean text results. This will be determined by \n",
        "the ratio of dictionary words and other expected tokens to the total text results. Specimen images producing high\n",
        "quality results will be used for further analysis and comparision to the human parsed text.\n",
        "\n",
        "Analysis of OCR and Human Parsed Text\n",
        "The raw OCR text of each specimen will be analyized and compared to the human parsed text in the structured Darwin Core \n",
        "fields in each specimen record. Using various NLP methods, the OCR text will be analyzed to determine the success rate\n",
        "of extracting key textual components including named entities, dates, geographic coordinates and other data which can be\n",
        "correlated with the structured Darwin Core metadata. \n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOG0OPpJ7-52"
      },
      "source": [
        "Question 2 (12 points): Write python code to collect 500 items of the data you plan to collect above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxyXAFUg7-52",
        "outputId": "d7fa775d-d69d-4eee-839a-8f7be40fc066"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Retrieve CSV/ZIP dataset from a Symbiota portal\n",
        "\n",
        "This script downloads botanical specimen data from data portals which are based\n",
        "on the Symbiota web application.\n",
        "\n",
        "The search and download process generally starts at the collection search interface:\n",
        "https://portal.torcherbaria.org/portal/collections/list.php\n",
        "This is where the collection(s) is/are selected which provide the db value(s) which \n",
        "are hidden in the download form.\n",
        "The search criteria form:\n",
        "https://portal.torcherbaria.org/portal/collections/harvestparams.php\n",
        "is where other parameters are input (taxonomy, geography, etc)\n",
        "The results page:\n",
        "https://portal.torcherbaria.org/portal/collections/list.php\n",
        "displays a list of the matches. On this page is the download link which opens\n",
        "the download form and the form handler which is used by this script.\n",
        "\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "url = 'https://portal.torcherbaria.org/portal/collections/download/downloadhandler.php'\n",
        "schema = 'dwc' # Darwin Core\n",
        "file_format = 'csv' #form field name is 'format'\n",
        "cset = 'utf-8'\n",
        "publicsearch = '1'\n",
        "taxonFilterCode = '0'\n",
        "sourcepage = 'specimen'\n",
        "images = '1' # include images - 1, only includes image records\n",
        "zip_file = '0' # form field name is zip, default to not zip file\n",
        "if images == '1': # results must be zipped to get both specimen and image records\n",
        "  zip_file = '1' # form field name is zip\n",
        "# db=370 specifies the BRIT dataset, this can be changed to specify other datasets\n",
        "searchvar = 'db=370&state=Texas&county=Tarrant&hasimages=1'\n",
        "\n",
        "r = requests.post(url, data={'schema': schema, 'format': file_format, \n",
        "    'cset': cset, 'publicsearch': publicsearch, \n",
        "    'taxonFilterCode': taxonFilterCode, \n",
        "    'images': images, 'zip':zip_file,\n",
        "    'sourcepage': sourcepage, 'searchvar': searchvar},\n",
        "    stream=True)\n",
        "\n",
        "# Save data\n",
        "if zip_file == '0':\n",
        "  #Save CSV\n",
        "  filename = 'symbiota_data.csv'\n",
        "  with open(filename, 'w') as data_file:\n",
        "    data_file.write(r.text)\n",
        "  print(f'File {filename} saved.')\n",
        "\n",
        "if zip_file =='1':\n",
        "  # Save ZIP\n",
        "  filename = 'symbiota_data.zip'\n",
        "  with open(filename, 'wb') as zip_file:\n",
        "    for chunk in r.iter_content(chunk_size=128):\n",
        "      zip_file.write(chunk)\n",
        "  print(f'File {filename} saved.')\n",
        "  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File symbiota_data.zip saved.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}